# 신조어 추출 알고리즘 상세 설명

## 질문에 대한 답변

**Q: 신조어 추출에 사용되는 알고리즘은 CNN인가요? NER인가요? NLP인가요?**

**A: NLP(자연어 처리) 분야의 통계적 알고리즘입니다.**

## 용어 구분

### 1. NLP (Natural Language Processing) - 자연어 처리
- **정의**: 컴퓨터가 인간의 언어를 이해하고 처리하는 전체 분야
- **범위**: 가장 넓은 개념
- **예시**: 번역, 감정 분석, 텍스트 생성, 형태소 분석 등
- **우리 시스템**: ✅ **NLP에 속함**

### 2. NER (Named Entity Recognition) - 개체명 인식
- **정의**: 텍스트에서 특정 개체(사람, 장소, 조직 등)를 찾아내는 작업
- **목적**: "홍길동은 서울에서 삼성전자에 다닌다" → (사람: 홍길동, 장소: 서울, 조직: 삼성전자)
- **우리 시스템**: ❌ **NER이 아님** (개체명이 아닌 신조어 추출)

### 3. CNN (Convolutional Neural Network) - 합성곱 신경망
- **정의**: 딥러닝의 한 종류, 주로 이미지 처리에 사용
- **NLP 적용**: 텍스트 분류에도 사용 가능하지만...
- **우리 시스템**: ❌ **CNN이 아님** (딥러닝 미사용)

## 우리 시스템에서 사용하는 알고리즘

### 🎯 핵심: **통계적 단어 추출 (Unsupervised Learning)**

```
분류: NLP > 단어 분할 > 통계적 방법
방법론: soynlp WordExtractor (비지도 학습)
```

## 알고리즘 상세

### 1. 전체 흐름

```python
텍스트 수집
    ↓
전처리 (정규화, 노이즈 제거)
    ↓
통계적 분석 (soynlp WordExtractor)
    ↓
후보 단어 추출
    ↓
필터링 (빈도, 응집도, 엔트로피)
    ↓
점수 계산 및 순위화
    ↓
신조어 리스트
```

### 2. soynlp WordExtractor 알고리즘

#### (1) 응집도 (Cohesion)
**정의**: 단어 내부의 글자들이 얼마나 강하게 결합되어 있는가?

```
예시: "갓생"
- "갓" + "생" = 자주 함께 출현 → 높은 응집도
- P(갓생) / (P(갓) × P(생)) 비율 계산
```

**수식**:
```
cohesion(w) = P(w) / (P(w[0]) × P(w[1]) × ... × P(w[n]))
```

#### (2) 분기 엔트로피 (Branching Entropy)
**정의**: 단어의 경계를 판단하기 위해 좌우 문맥의 다양성을 측정

```
예시: "점메추"
- 왼쪽 문맥: "오늘 점메추", "내일 점메추", "지금 점메추" → 다양함
- 오른쪽 문맥: "점메추 해줘", "점메추 해요" → 다양함
→ 독립적인 단어로 판단
```

**수식** (Shannon Entropy):
```
H = -Σ P(x) × log(P(x))
```
- 다양성이 높을수록 → 높은 엔트로피 → 단어 경계

#### (3) 빈도 기반 필터링
```python
min_count = 5  # 최소 5번 이상 출현한 후보만
```

### 3. 형태소 분석 보조 (kiwipiepy)

**용도**: 신조어 유형 분류

```python
def classify_neologism_type(word):
    # 축약어: ㅇㅈ, ㄱㅅ (자음만)
    if re.match(r'^[ㄱ-ㅎ]+$', word):
        return 'abbreviation'

    # 합성어: 형태소 분석 결과가 2개 이상
    if len(kiwi.tokenize(word)) > 1:
        return 'compound'
```

### 4. 점수 계산 (우리 커스텀 알고리즘)

```python
score = (
    0.3 × cohesion +           # 응집도
    0.3 × right_entropy +      # 우측 엔트로피
    0.2 × left_entropy +       # 좌측 엔트로피
    0.2 × log(frequency) / 10  # 정규화된 빈도
)
```

**가중치 설명**:
- 응집도 30%: 단어의 내부 결합력
- 우측/좌측 엔트로피 50%: 단어 경계 명확성
- 빈도 20%: 실제 사용 빈도

## 알고리즘 비교표

| 특징 | 통계적 방법 (우리) | 딥러닝 (CNN/RNN) | NER |
|------|-------------------|------------------|-----|
| **학습 데이터** | 레이블 불필요 | 레이블 필요 | 레이블 필요 |
| **계산 비용** | 낮음 | 높음 | 중간 |
| **설명 가능성** | 높음 | 낮음 | 중간 |
| **신조어 발견** | 우수 | 보통 | 부적합 |
| **속도** | 빠름 | 느림 | 중간 |

## 왜 CNN이나 딥러닝을 사용하지 않았나?

### 통계적 방법의 장점

1. **비지도 학습**: 레이블 데이터 불필요
   - 신조어는 계속 생성되므로 레이블링 불가능

2. **설명 가능성**: 왜 이 단어가 신조어인지 명확
   - 응집도: 0.7, 빈도: 150회 → 근거 명확

3. **빠른 처리**: 실시간 처리 가능
   - Glue Job 2 DPU로 충분

4. **한국어 최적화**: soynlp는 한국어 특성에 맞춤
   - 교착어 특성 반영
   - 띄어쓰기 오류에 강건

### 딥러닝이 필요한 경우

- **문맥 이해 필요**: 감정 분석, 번역 등
- **복잡한 패턴**: 장문 요약, 대화 생성 등
- **대규모 레이블 데이터 존재**: 이미지 분류 등

→ 신조어 추출은 통계적 방법이 더 적합!

## 실제 예시

### 입력 텍스트
```
"오늘 완전 꿀잼이었어 ㅋㅋㅋ"
"점메추 좀 해주세요"
"갓생 살고 싶다"
```

### 처리 과정

#### 1단계: 전처리
```
"오늘 완전 꿀잼이었어 ㅋㅋ"  (반복 문자 정규화)
"점메추 좀 해주세요"
"갓생 살고 싶다"
```

#### 2단계: 통계 계산
```
"꿀잼" 출현 횟수: 5회
P(꿀잼) = 5/100
P(꿀) = 8/100, P(잼) = 10/100
cohesion = (5/100) / ((8/100) × (10/100)) = 6.25 → 높음!
```

#### 3단계: 엔트로피 계산
```
"꿀잼" 좌측 문맥: ["완전", "진짜", "너무"] → 다양함
"꿀잼" 우측 문맥: ["이었어", "이야", "임"] → 다양함
→ 독립된 단어로 판단
```

#### 4단계: 점수 및 결과
```json
{
  "word": "꿀잼",
  "score": 0.78,
  "frequency": 5,
  "cohesion": 6.25,
  "type": "compound"
}
```

## 결론

**우리의 신조어 추출 시스템은:**
- ✅ **NLP** 분야
- ✅ **통계적 비지도 학습** 알고리즘
- ✅ **soynlp WordExtractor** + **형태소 분석**
- ❌ CNN (딥러닝) 아님
- ❌ NER (개체명 인식) 아님

**선택 이유:**
- 레이블 데이터 불필요
- 빠른 처리 속도
- 높은 설명 가능성
- 한국어 신조어에 최적화

## 참고 문헌

- Park, E. L. (2018). "soynlp: Python library for Korean text mining"
- Lovins, J. B. (1968). "Development of a stemming algorithm"
- Shannon, C. E. (1948). "A Mathematical Theory of Communication"
