# ê²€ìƒ‰ ì—”ì§„ í†µí•© ê°€ì´ë“œ

ì‹ ì¡°ì–´ ì‚¬ì „ì„ OpenSearch/Elasticsearchì— í†µí•©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.

## ğŸ“‹ ëª©ì°¨

- [ê°œìš”](#ê°œìš”)
- [ë™ì˜ì–´ ìƒì„± ì•Œê³ ë¦¬ì¦˜](#ë™ì˜ì–´-ìƒì„±-ì•Œê³ ë¦¬ì¦˜)
- [ìƒì„±ë˜ëŠ” íŒŒì¼](#ìƒì„±ë˜ëŠ”-íŒŒì¼)
- [OpenSearch/Elasticsearch ì„¤ì •](#opensearchelasticsearch-ì„¤ì •)
- [ì‚¬ìš© ì˜ˆì œ](#ì‚¬ìš©-ì˜ˆì œ)
- [FAQ](#faq)

## ğŸ¯ ê°œìš”

ì‹ ì¡°ì–´ ì¶”ì¶œ íŒŒì´í”„ë¼ì¸ì€ ê²€ìƒ‰ ì—”ì§„(OpenSearch/Elasticsearch)ì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ê²°ê³¼ë¥¼ ì œê³µí•©ë‹ˆë‹¤:

1. **ë™ì˜ì–´ íŒŒì¼** - ìœ ì‚¬í•œ ì‹ ì¡°ì–´ë“¤ì„ ë™ì˜ì–´ë¡œ ê·¸ë£¹í™”
2. **ì‚¬ìš©ì ì‚¬ì „** - Nori Tokenizerê°€ ì¸ì‹í•  ìˆ˜ ìˆëŠ” í˜•íƒœ
3. **ì¸ë±ìŠ¤ ì„¤ì •** - analyzer, tokenizer, filter ì„¤ì • í¬í•¨

### ì£¼ìš” ê¸°ëŠ¥

âœ… **ìë™ ë™ì˜ì–´ ìƒì„±**
- í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜ (ë³€í˜•ì–´ ê°ì§€)
- í˜•íƒœì†Œ íŒ¨í„´ ê¸°ë°˜ (ì¤„ì„ë§ êµ¬ì¡° ìœ ì‚¬ë„)
- ê³µê¸°ì–´ ë¶„ì„ (ë¬¸ë§¥ ê¸°ë°˜ ìœ ì‚¬ë„)

âœ… **ê²€ìƒ‰ ì—”ì§„ í¬ë§· ìë™ ë³€í™˜**
- Solr ë™ì˜ì–´ í˜•ì‹
- WordNet ë™ì˜ì–´ í˜•ì‹
- Nori ì‚¬ìš©ì ì‚¬ì „ í˜•ì‹

âœ… **ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„¤ì • íŒŒì¼**
- ì¸ë±ìŠ¤ settings & mappings
- analyzer ë° tokenizer êµ¬ì„±
- ì„¤ì • ìŠ¤í¬ë¦½íŠ¸

---

## ğŸ§® ë™ì˜ì–´ ìƒì„± ì•Œê³ ë¦¬ì¦˜

### 1. í¸ì§‘ ê±°ë¦¬ ê¸°ë°˜ (Levenshtein Distance)

**ëª©ì :** ë³€í˜•ì–´ ë° ì˜¤íƒ€ ê°ì§€

```python
# ì˜ˆì‹œ
"ê°“ìƒ" â†” "ê°“ìƒí™œ"  (ìœ ì‚¬ë„: 0.75)
"ì ë©”ì¶”" â†” "ì ë©”ë‰´ì¶”"  (ìœ ì‚¬ë„: 0.70)
```

**ì•Œê³ ë¦¬ì¦˜:**
- SequenceMatcherë¡œ ìœ ì‚¬ë„ ê³„ì‚°
- ì„ê³„ê°’ 0.7 ì´ìƒì¸ ê²½ìš° ë™ì˜ì–´ë¡œ ê°„ì£¼
- ê¸¸ì´ ì°¨ì´ê°€ 2 ì´í•˜ì¸ ë‹¨ì–´ë§Œ ë¹„êµ

**ì¥ì :**
- ë¹ ë¥¸ ì²˜ë¦¬ ì†ë„
- ê°„ë‹¨í•œ êµ¬í˜„
- ì˜¤íƒ€ ë° ë³€í˜•ì–´ì— íš¨ê³¼ì 

### 2. í˜•íƒœì†Œ íŒ¨í„´ ê¸°ë°˜

**ëª©ì :** ì¤„ì„ë§ì˜ êµ¬ì¡°ì  ìœ ì‚¬ì„± ê°ì§€

```python
# ì˜ˆì‹œ
"ì ë©”ì¶”" (ì ì‹¬+ë©”ë‰´+ì¶”ì²œ) â†” "ì €ë©”ì¶”" (ì €ë…+ë©”ë‰´+ì¶”ì²œ)
ì´ˆì„± íŒ¨í„´: ã…ˆã…ã…Š = ã…ˆã…ã…Š â†’ ìœ ì‚¬
```

**ì•Œê³ ë¦¬ì¦˜:**
- í•œê¸€ ì´ˆì„± ì¶”ì¶œ
- ì´ˆì„± íŒ¨í„´ ë¹„êµ
- 50% ì´ìƒ ì¼ì¹˜í•˜ë©´ ìœ ì‚¬

**ì¥ì :**
- í•œêµ­ì–´ íŠ¹ì„±ì— ìµœì í™”
- ì¤„ì„ë§ íŒ¨í„´ ì¸ì‹

### 3. ê³µê¸°ì–´ ë¶„ì„ (Co-occurrence)

**ëª©ì :** ê°™ì€ ë¬¸ë§¥ì—ì„œ ì‚¬ìš©ë˜ëŠ” ë‹¨ì–´ ê°ì§€

```python
# ì˜ˆì‹œ
"ê¿€ì¼"ê³¼ "í•µì¼"ì´ ê°™ì€ ë¬¸ì¥ì— ìì£¼ ë“±ì¥
â†’ ìœ ì‚¬í•œ ì˜ë¯¸ë¡œ ì‚¬ìš©ë¨ì„ ì¶”ë¡ 
```

**ì•Œê³ ë¦¬ì¦˜:**
- ê°™ì€ ë¬¸ì¥ ë‚´ ë™ì‹œ ì¶œí˜„ ë¹ˆë„ ê³„ì‚°
- ì„ê³„ê°’(ê¸°ë³¸ 3íšŒ) ì´ìƒì´ë©´ ë™ì˜ì–´ë¡œ ê°„ì£¼

**ì¥ì :**
- ì˜ë¯¸ ê¸°ë°˜ ìœ ì‚¬ë„
- ì‚¬ìš© íŒ¨í„´ ë°˜ì˜

### 4. ì˜ë¯¸ ì„ë² ë”© (ì„ íƒ ì‚¬í•­)

**ëª©ì :** Semantic similarity ê³„ì‚°

```python
# sentence-transformers ì‚¬ìš©
model = SentenceTransformer('jhgan/ko-sroberta-multitask')
embeddings = model.encode(words)
similarity = cosine_similarity(embeddings)
```

**ì•Œê³ ë¦¬ì¦˜:**
- BERT ê¸°ë°˜ ì„ë² ë”© ìƒì„±
- ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
- ì„ê³„ê°’ 0.7 ì´ìƒì´ë©´ ë™ì˜ì–´

**ì¥ì :**
- ê°€ì¥ ì •í™•í•œ ì˜ë¯¸ ìœ ì‚¬ë„
- ë§¥ë½ ì´í•´

**ë‹¨ì :**
- ëŠë¦° ì²˜ë¦¬ ì†ë„
- ë†’ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©

### ì¢…í•© ì „ëµ

**ê¸°ë³¸ ì„¤ì • (ë¹ ë¥´ê³  íš¨ê³¼ì ):**
```python
SynonymGenerator(
    edit_distance_threshold=0.7,    # í¸ì§‘ ê±°ë¦¬
    cooccurrence_threshold=3,        # ê³µê¸°ì–´ ìµœì†Œ ë¹ˆë„
    use_semantic=False               # ì˜ë¯¸ ì„ë² ë”© ë¯¸ì‚¬ìš©
)
```

**ê³ ê¸‰ ì„¤ì • (ì •í™•í•˜ì§€ë§Œ ëŠë¦¼):**
```python
SynonymGenerator(
    edit_distance_threshold=0.7,
    cooccurrence_threshold=3,
    use_semantic=True                # ì˜ë¯¸ ì„ë² ë”© ì‚¬ìš©
)
```

---

## ğŸ“¦ ìƒì„±ë˜ëŠ” íŒŒì¼

### 1. synonyms.txt (Solr í˜•ì‹)

```
ê°“ìƒ, ê°“ìƒí™œ
ê¿€ì¼, í•µì¼
ì ë©”ì¶”, ì €ë©”ì¶”
```

**ìš©ë„:** OpenSearch/Elasticsearch synonym filter

**ì„¤ì •:**
```json
{
  "filter": {
    "synonym_filter": {
      "type": "synonym",
      "synonyms_path": "synonyms.txt"
    }
  }
}
```

### 2. synonyms_wordnet.txt (WordNet í˜•ì‹)

```
ê°“ìƒ => ê°“ìƒí™œ
ê¿€ì¼ => í•µì¼
ì ë©”ì¶” => ì €ë©”ì¶”, ì ì‹¬ë©”ë‰´ì¶”ì²œ
```

**ìš©ë„:** ë°©í–¥ì„±ì´ ìˆëŠ” ë™ì˜ì–´ ë§¤í•‘

### 3. user_dictionary.txt (ê¸°ë³¸ ì‚¬ìš©ì ì‚¬ì „)

```
ê°“ìƒ
ê¿€ì¼
ì ë©”ì¶”
í•µì¸ì‹¸
JMT
```

**ìš©ë„:** ê¸°ë³¸ í† í¬ë‚˜ì´ì €ìš© ì‚¬ìš©ì ì‚¬ì „

### 4. nori_user_dictionary.txt (Nori Tokenizerìš©)

```
ê°“ìƒ NNG
ê¿€ì¼ NNG
ì ë©”ì¶” NNP
í•µì¸ì‹¸ NNG
JMT NNP
```

**ìš©ë„:** Nori Tokenizerìš© ì‚¬ìš©ì ì‚¬ì „ (í’ˆì‚¬ íƒœê·¸ í¬í•¨)

**í’ˆì‚¬ íƒœê·¸:**
- `NNG`: ì¼ë°˜ëª…ì‚¬ (compound, slang)
- `NNP`: ê³ ìœ ëª…ì‚¬ (abbreviation)

### 5. index_settings.json (ì¸ë±ìŠ¤ ì„¤ì •)

ì™„ì „í•œ OpenSearch/Elasticsearch ì¸ë±ìŠ¤ ì„¤ì •:

```json
{
  "settings": {
    "analysis": {
      "tokenizer": {
        "nori_user_dict": {
          "type": "nori_tokenizer",
          "decompound_mode": "mixed",
          "user_dictionary": "nori_user_dictionary.txt"
        }
      },
      "filter": {
        "synonym_filter": {
          "type": "synonym",
          "synonyms_path": "synonyms.txt",
          "updateable": true
        }
      },
      "analyzer": {
        "korean_analyzer": {
          "type": "custom",
          "tokenizer": "nori_user_dict",
          "filter": ["lowercase", "synonym_filter", "nori_readingform"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "text": {
        "type": "text",
        "analyzer": "korean_analyzer"
      }
    }
  }
}
```

### 6. README.md

ìì„¸í•œ ì‚¬ìš© ë°©ë²• ë° ì„¤ëª…

### 7. setup_index.sh

ìë™í™” ìŠ¤í¬ë¦½íŠ¸:
```bash
./setup_index.sh https://localhost:9200
```

---

## ğŸ”§ OpenSearch/Elasticsearch ì„¤ì •

### ë°©ë²• 1: ìë™ ì„¤ì • (ê¶Œì¥)

```bash
# 1. S3ì—ì„œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
aws s3 cp s3://your-bucket/output/search_engine/latest/ . --recursive

# 2. ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
chmod +x setup_index.sh
./setup_index.sh https://your-opensearch-url:9200

# 3. íŒŒì¼ ë°°ì¹˜ (ìˆ˜ë™)
cp synonyms.txt /etc/opensearch/analysis/
cp nori_user_dictionary.txt /etc/opensearch/

# 4. OpenSearch ì¬ì‹œì‘
sudo systemctl restart opensearch
```

### ë°©ë²• 2: ìˆ˜ë™ ì„¤ì •

#### Step 1: íŒŒì¼ ë°°ì¹˜

```bash
# OpenSearch
/etc/opensearch/
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ synonyms.txt
â”œâ”€â”€ nori_user_dictionary.txt

# Elasticsearch
/etc/elasticsearch/
â”œâ”€â”€ analysis/
â”‚   â””â”€â”€ synonyms.txt
â”œâ”€â”€ nori_user_dictionary.txt
```

#### Step 2: ì¸ë±ìŠ¤ ìƒì„±

```bash
curl -X PUT "localhost:9200/neologism_search" \
  -H 'Content-Type: application/json' \
  -d @index_settings.json
```

#### Step 3: ë™ì˜ì–´ ì—…ë°ì´íŠ¸ (ëŸ°íƒ€ì„)

```bash
# synonyms.txt ìˆ˜ì • í›„
curl -X POST "localhost:9200/neologism_search/_reload_search_analyzers"
```

### ë°©ë²• 3: AWS OpenSearch Service

```bash
# 1. S3ì—ì„œ íŒ¨í‚¤ì§€ ì—…ë¡œë“œ
aws opensearch upload-package \
  --package-name neologism-synonyms \
  --package-type TXT-DICTIONARY \
  --package-source S3BucketName=your-bucket,S3Key=search_engine/latest/synonyms.txt

# 2. íŒ¨í‚¤ì§€ ì—°ê²°
aws opensearch associate-package \
  --package-id package-id \
  --domain-name your-domain
```

---

## ğŸ’¡ ì‚¬ìš© ì˜ˆì œ

### ë¡œì»¬ Python ìŠ¤í¬ë¦½íŠ¸

```python
from neologism_extractor import (
    NeologismExtractor,
    SynonymGenerator,
    SearchEngineExporter
)

# 1. ì‹ ì¡°ì–´ ì¶”ì¶œ
extractor = NeologismExtractor()
neologisms = extractor.extract_neologisms(texts)

# 2. ë™ì˜ì–´ ìƒì„±
synonym_gen = SynonymGenerator()
synonyms = synonym_gen.generate_synonyms(neologisms, texts)
groups = synonym_gen.generate_synonym_groups(synonyms)

# 3. ê²€ìƒ‰ ì—”ì§„ íŒŒì¼ ìƒì„±
exporter = SearchEngineExporter(output_dir="./search_exports")
files = exporter.export_all(neologisms, groups, synonyms)

print(f"ìƒì„±ëœ íŒŒì¼: {files}")
```

### AWS Glue Job (ìë™)

Glue Jobì€ ê¸°ë³¸ì ìœ¼ë¡œ ê²€ìƒ‰ ì—”ì§„ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤:

```python
# Glue Job íŒŒë¼ë¯¸í„°
--GENERATE_SYNONYMS=true
--EXPORT_FOR_SEARCH_ENGINE=true
```

ê²°ê³¼ëŠ” S3ì— ìë™ ì €ì¥:
```
s3://your-bucket/output/search_engine/latest/
â”œâ”€â”€ synonyms.txt
â”œâ”€â”€ synonyms_wordnet.txt
â”œâ”€â”€ user_dictionary.txt
â”œâ”€â”€ nori_user_dictionary.txt
â”œâ”€â”€ index_settings.json
â””â”€â”€ README.md
```

### ê²€ìƒ‰ í…ŒìŠ¤íŠ¸

```bash
# ì¸ë±ìŠ¤ í…ŒìŠ¤íŠ¸
curl -X GET "localhost:9200/neologism_search/_analyze" \
  -H 'Content-Type: application/json' \
  -d '{
    "analyzer": "korean_analyzer",
    "text": "ê°“ìƒ ì‚´ê³  ì‹¶ì–´ìš”"
  }'

# ë™ì˜ì–´ í™•ì¸ (ê°“ìƒ â†’ ê°“ìƒí™œë„ ê²€ìƒ‰ë¨)
curl -X GET "localhost:9200/neologism_search/_search" \
  -H 'Content-Type: application/json' \
  -d '{
    "query": {
      "match": {
        "text": "ê°“ìƒ"
      }
    }
  }'
```

---

## ğŸ” FAQ

### Q1. ë™ì˜ì–´ê°€ ë„ˆë¬´ ë§ì´ ìƒì„±ë˜ëŠ”ë° ì–´ë–»ê²Œ ì œì–´í•˜ë‚˜ìš”?

**A:** `SynonymGenerator`ì˜ ì„ê³„ê°’ì„ ì¡°ì •í•˜ì„¸ìš”:

```python
synonym_gen = SynonymGenerator(
    edit_distance_threshold=0.8,  # ë” ì—„ê²©í•˜ê²Œ (ê¸°ë³¸: 0.7)
    cooccurrence_threshold=5       # ë” ë†’ì€ ë¹ˆë„ ìš”êµ¬ (ê¸°ë³¸: 3)
)
```

### Q2. ë™ì˜ì–´ê°€ ë„ˆë¬´ ì ê²Œ ìƒì„±ë˜ëŠ”ë°ìš”?

**A:** ì„ê³„ê°’ì„ ë‚®ì¶”ê±°ë‚˜ ì˜ë¯¸ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì„¸ìš”:

```python
synonym_gen = SynonymGenerator(
    edit_distance_threshold=0.6,  # ë” ê´€ëŒ€í•˜ê²Œ
    use_semantic=True              # ì˜ë¯¸ ê¸°ë°˜ ì¶”ê°€
)
```

### Q3. OpenSearchì—ì„œ ë™ì˜ì–´ê°€ ì‘ë™í•˜ì§€ ì•Šì•„ìš”

**A:** ì²´í¬ë¦¬ìŠ¤íŠ¸:
1. íŒŒì¼ ê²½ë¡œê°€ ì˜¬ë°”ë¥¸ì§€ í™•ì¸
2. OpenSearch ì¬ì‹œì‘í–ˆëŠ”ì§€ í™•ì¸
3. ì¸ë±ìŠ¤ ì¬ìƒì„± ë˜ëŠ” reload analyzer ì‹¤í–‰
4. ë¡œê·¸ í™•ì¸: `/var/log/opensearch/`

```bash
# Analyzer ì¬ë¡œë“œ
curl -X POST "localhost:9200/neologism_search/_reload_search_analyzers"
```

### Q4. ì‹¤ì‹œê°„ìœ¼ë¡œ ë™ì˜ì–´ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì‹¶ì–´ìš”

**A:** `updateable: true` ì„¤ì • í™•ì¸ í›„ reload API ì‚¬ìš©:

```json
{
  "filter": {
    "synonym_filter": {
      "type": "synonym",
      "synonyms_path": "synonyms.txt",
      "updateable": true  // ì¤‘ìš”!
    }
  }
}
```

```bash
# íŒŒì¼ ì—…ë°ì´íŠ¸ í›„
curl -X POST "localhost:9200/neologism_search/_reload_search_analyzers"
```

### Q5. Nori Tokenizerê°€ ì‹ ì¡°ì–´ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•´ìš”

**A:** ì‚¬ìš©ì ì‚¬ì „ ê²½ë¡œì™€ ì¬ì‹œì‘ í™•ì¸:

```bash
# 1. íŒŒì¼ í™•ì¸
ls -la /etc/opensearch/nori_user_dictionary.txt

# 2. ê¶Œí•œ í™•ì¸
chmod 644 /etc/opensearch/nori_user_dictionary.txt

# 3. ì¬ì‹œì‘
sudo systemctl restart opensearch

# 4. í† í¬ë‚˜ì´ì € í…ŒìŠ¤íŠ¸
curl -X GET "localhost:9200/neologism_search/_analyze" \
  -d '{"tokenizer": "nori_user_dict", "text": "ê°“ìƒ"}'
```

### Q6. AWS OpenSearch Serviceì—ì„œëŠ” ì–´ë–»ê²Œ ì‚¬ìš©í•˜ë‚˜ìš”?

**A:** S3 íŒ¨í‚¤ì§€ ì—…ë¡œë“œ ë°©ì‹ ì‚¬ìš©:

```bash
# 1. íŒ¨í‚¤ì§€ ìƒì„±
aws opensearch create-package \
  --package-name neologism-dict \
  --package-type TXT-DICTIONARY \
  --package-source S3BucketName=your-bucket,S3Key=search_engine/latest/synonyms.txt

# 2. ë„ë©”ì¸ì— ì—°ê²°
aws opensearch associate-package \
  --package-id F123456789 \
  --domain-name your-domain

# 3. ì¸ë±ìŠ¤ ì„¤ì •ì—ì„œ ì°¸ì¡°
{
  "filter": {
    "synonym_filter": {
      "type": "synonym_graph",
      "synonyms_path": "analyzers/neologism-dict"
    }
  }
}
```

### Q7. ì„±ëŠ¥ì— ì˜í–¥ì´ ìˆë‚˜ìš”?

**A:** ë™ì˜ì–´ ê°œìˆ˜ì— ë”°ë¼ ë‹¤ë¦…ë‹ˆë‹¤:

- **< 1,000ê°œ:** ê±°ì˜ ì˜í–¥ ì—†ìŒ
- **1,000 ~ 10,000ê°œ:** ì•½ê°„ì˜ ì„±ëŠ¥ ì €í•˜ (5-10%)
- **> 10,000ê°œ:** ëˆˆì— ë„ëŠ” ì €í•˜ (10-20%)

**ìµœì í™” íŒ:**
- ìì£¼ ì‚¬ìš©í•˜ëŠ” ë™ì˜ì–´ë§Œ í¬í•¨
- `synonym_graph` ëŒ€ì‹  `synonym` ì‚¬ìš©
- ì¸ë±ìŠ¤ timeì´ ì•„ë‹Œ query timeì— ì ìš©

---

## ğŸ“š ì°¸ê³  ìë£Œ

- [OpenSearch Synonym Token Filter](https://opensearch.org/docs/latest/analyzers/token-filters/synonym/)
- [Elasticsearch Nori Tokenizer](https://www.elastic.co/guide/en/elasticsearch/plugins/current/analysis-nori.html)
- [AWS OpenSearch Custom Packages](https://docs.aws.amazon.com/opensearch-service/latest/developerguide/custom-packages.html)

---

## ğŸ†˜ ë¬¸ì œ í•´ê²°

ë¬¸ì œê°€ ë°œìƒí•˜ë©´:

1. **ë¡œê·¸ í™•ì¸**
   ```bash
   tail -f /var/log/opensearch/opensearch.log
   ```

2. **ì„¤ì • í™•ì¸**
   ```bash
   curl -X GET "localhost:9200/neologism_search/_settings?pretty"
   ```

3. **Analyzer í…ŒìŠ¤íŠ¸**
   ```bash
   curl -X GET "localhost:9200/neologism_search/_analyze?pretty" \
     -d '{"analyzer": "korean_analyzer", "text": "í…ŒìŠ¤íŠ¸"}'
   ```

4. **GitHub Issues**: https://github.com/your-repo/issues
