{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 신조어 추출 및 코퍼스 생성 테스트\n",
    "\n",
    "이 노트북은 공공 데이터(SNS, AIHub)에서 신조어를 추출하고 코퍼스를 생성하는 전체 파이프라인을 테스트합니다.\n",
    "\n",
    "## 목차\n",
    "1. 환경 설정\n",
    "2. 데이터 수집\n",
    "3. 신조어 추출\n",
    "4. 코퍼스 생성\n",
    "5. 결과 분석 및 시각화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 환경 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "print(\"환경 설정 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 우리 패키지 임포트\n",
    "from neologism_extractor import NeologismExtractor, DataCollector, CorpusBuilder\n",
    "from neologism_extractor.data_collector import TwitterCollector, AIHubCollector\n",
    "\n",
    "print(\"패키지 임포트 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 수집\n",
    "\n",
    "다양한 소스에서 텍스트 데이터를 수집합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 수집기 초기화\n",
    "collector = DataCollector()\n",
    "\n",
    "# Twitter 수집기 등록 (샘플 데이터 사용)\n",
    "twitter_collector = TwitterCollector()\n",
    "collector.register_collector('twitter', twitter_collector)\n",
    "\n",
    "# 데이터 수집\n",
    "print(\"데이터 수집 중...\")\n",
    "all_texts = collector.collect_and_merge(max_results=100)\n",
    "\n",
    "print(f\"\\n총 {len(all_texts)}개의 텍스트 수집 완료!\")\n",
    "print(\"\\n샘플 데이터:\")\n",
    "for i, text in enumerate(all_texts[:5], 1):\n",
    "    print(f\"{i}. {text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샘플 데이터 추가 (테스트용)\n",
    "\n",
    "실제 API가 없을 경우를 대비해 샘플 데이터를 추가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플 한국어 텍스트 (신조어 포함)\n",
    "sample_texts = [\n",
    "    \"오늘 완전 꿀잼이었어 ㅋㅋㅋ\",\n",
    "    \"점메추 좀 해주세요\",\n",
    "    \"갓생 살고 싶다 진짜\",\n",
    "    \"이거 레알 대박이네요\",\n",
    "    \"오늘 TMI 하나 풀자면 말이야\",\n",
    "    \"완전 핵인싸네 ㄷㄷ\",\n",
    "    \"JMT 맛집 발견했어요!\",\n",
    "    \"오늘 할일 다 끝! 갓생이다\",\n",
    "    \"이번주 불금이다 와우\",\n",
    "    \"월급루팡 중입니다\",\n",
    "    \"오늘 회사에서 칼퇴했어요\",\n",
    "    \"주말에 홈캉스 어때?\",\n",
    "    \"완전 워라밸 좋은 회사\",\n",
    "    \"점메추 진짜 어려워요\",\n",
    "    \"꾸꾸까까 하고 싶다\",\n",
    "    \"이거 실화냐 레전드네\",\n",
    "    \"갓생 루틴 시작!\",\n",
    "    \"오늘 점심 뭐먹지 점메추\",\n",
    "    \"주말에 뭐하지 주말추천\",\n",
    "    \"불금에 치맥 어때요?\",\n",
    "    \"갓벽한 하루였다\",\n",
    "    \"완전 취저입니다\",\n",
    "    \"이건 찐이다 진짜\",\n",
    "    \"오늘 완전 점메추 실패\",\n",
    "    \"주말에 숲캉스 가고 싶어\",\n",
    "    \"일코 지키기 힘들다\",\n",
    "    \"오늘도 갓생 살기\",\n",
    "    \"완전 꿀잼 영화였어\",\n",
    "    \"너무 핵노잼이야\",\n",
    "    \"오늘 저녁은 혼밥\",\n",
    "    \"완전 대박이네요 이거\",\n",
    "    \"레알 찐이에요\",\n",
    "    \"갓생 루틴 추천해주세요\",\n",
    "    \"오늘은 완전 점심 고민\",\n",
    "    \"불금에 치맥 최고\"\n",
    "]\n",
    "\n",
    "all_texts.extend(sample_texts * 5)  # 빈도를 높이기 위해 5번 반복\n",
    "\n",
    "print(f\"샘플 데이터 추가 후 총 {len(all_texts)}개의 텍스트\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 신조어 추출\n",
    "\n",
    "수집된 텍스트에서 신조어를 추출합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신조어 추출기 초기화\n",
    "extractor = NeologismExtractor(\n",
    "    min_count=3,           # 최소 3번 이상 출현\n",
    "    min_cohesion=0.03,     # 낮은 응집도 (더 많은 후보)\n",
    "    min_length=2,          # 최소 2글자\n",
    "    max_length=10          # 최대 10글자\n",
    ")\n",
    "\n",
    "print(\"신조어 추출 중...\")\n",
    "neologisms = extractor.extract_neologisms(all_texts)\n",
    "\n",
    "print(f\"\\n총 {len(neologisms)}개의 신조어 후보 추출 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 상위 신조어 확인\n",
    "print(\"\\n=== 상위 20개 신조어 ===\")\n",
    "print(f\"{'순위':<5} {'신조어':<15} {'점수':<10} {'빈도':<10} {'응집도':<10}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (word, stats) in enumerate(list(neologisms.items())[:20], 1):\n",
    "    print(f\"{i:<5} {word:<15} {stats['score']:<10.4f} {stats['count']:<10} {stats['cohesion']:<10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 문맥과 함께 신조어 추출\n",
    "print(\"\\n문맥 정보 추출 중...\")\n",
    "neologisms_with_context = extractor.extract_with_context(all_texts, top_n=50)\n",
    "\n",
    "print(\"\\n=== 신조어 예시 (문맥 포함) ===\")\n",
    "for item in neologisms_with_context[:10]:\n",
    "    print(f\"\\n단어: {item['word']}\")\n",
    "    print(f\"점수: {item['score']:.4f} | 빈도: {item['count']}\")\n",
    "    print(\"예시:\")\n",
    "    for example in item['examples'][:2]:\n",
    "        print(f\"  - {example}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 코퍼스 생성\n",
    "\n",
    "추출된 신조어로 코퍼스(사전)를 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코퍼스 빌더 초기화\n",
    "corpus_builder = CorpusBuilder(output_dir=\"../corpus\")\n",
    "\n",
    "# 신조어 유형 분류 추가\n",
    "for word, stats in neologisms.items():\n",
    "    stats['type'] = extractor.classify_neologism_type(word)\n",
    "\n",
    "# 사전 생성\n",
    "metadata = {\n",
    "    'source': 'Twitter, AIHub',\n",
    "    'collection_date': datetime.now().isoformat(),\n",
    "    'total_texts': len(all_texts)\n",
    "}\n",
    "\n",
    "dictionary = corpus_builder.build_dictionary(neologisms, metadata=metadata)\n",
    "\n",
    "print(f\"사전 생성 완료!\")\n",
    "print(f\"총 단어 수: {dictionary['total_words']}\")\n",
    "print(f\"생성 일시: {dictionary['created_at']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 형식으로 저장\n",
    "saved_files = corpus_builder.save_all_formats(dictionary, base_filename=\"neologism_dict\")\n",
    "\n",
    "print(\"\\n저장된 파일:\")\n",
    "for format_type, path in saved_files.items():\n",
    "    print(f\"  {format_type.upper()}: {path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 결과 분석 및 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame으로 변환\n",
    "df = pd.DataFrame(dictionary['words'])\n",
    "\n",
    "print(\"데이터 요약:\")\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통계 정보\n",
    "print(\"\\n=== 통계 정보 ===\")\n",
    "print(df[['score', 'frequency', 'cohesion']].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 신조어 유형별 분포\n",
    "print(\"\\n=== 신조어 유형별 분포 ===\")\n",
    "type_counts = df['type'].value_counts()\n",
    "print(type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 1: 상위 신조어 빈도\n",
    "plt.figure(figsize=(12, 6))\n",
    "top_words = df.head(20)\n",
    "plt.barh(range(len(top_words)), top_words['frequency'])\n",
    "plt.yticks(range(len(top_words)), top_words['word'])\n",
    "plt.xlabel('Frequency')\n",
    "plt.title('Top 20 Neologisms by Frequency')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 2: 점수 vs 빈도\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(df['frequency'], df['score'], alpha=0.5)\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Neologism Score vs Frequency')\n",
    "plt.xscale('log')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화 3: 유형별 분포\n",
    "if len(type_counts) > 0:\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    type_counts.plot(kind='pie', autopct='%1.1f%%')\n",
    "    plt.title('Neologism Type Distribution')\n",
    "    plt.ylabel('')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. S3 업로드 (선택사항)\n",
    "\n",
    "생성된 코퍼스를 S3에 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 업로드 설정 (필요시)\n",
    "S3_BUCKET = \"your-bucket-name\"  # 버킷 이름 변경\n",
    "S3_PREFIX = \"neologism-corpus/\"\n",
    "\n",
    "# 업로드 여부\n",
    "UPLOAD_TO_S3 = False  # True로 변경하면 업로드 실행\n",
    "\n",
    "if UPLOAD_TO_S3:\n",
    "    for format_type, local_path in saved_files.items():\n",
    "        s3_key = f\"{S3_PREFIX}{datetime.now().strftime('%Y%m%d')}/{local_path.split('/')[-1]}\"\n",
    "        try:\n",
    "            corpus_builder.upload_to_s3(local_path, S3_BUCKET, s3_key)\n",
    "        except Exception as e:\n",
    "            print(f\"업로드 실패 ({format_type}): {e}\")\n",
    "else:\n",
    "    print(\"S3 업로드 비활성화됨\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 완료!\n",
    "\n",
    "신조어 추출 및 코퍼스 생성이 완료되었습니다.\n",
    "\n",
    "생성된 파일:\n",
    "- JSON: 전체 메타데이터 포함\n",
    "- CSV: 엑셀에서 열람 가능\n",
    "- TXT: 단순 단어 리스트\n",
    "\n",
    "다음 단계:\n",
    "1. AWS Glue Job으로 자동화\n",
    "2. MWAA로 스케줄링\n",
    "3. 검색 엔진에 통합"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
